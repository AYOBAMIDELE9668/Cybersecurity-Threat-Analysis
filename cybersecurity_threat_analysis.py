# -*- coding: utf-8 -*-
"""Cybersecurity Threat Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IC2hajw6ct34BH5mhhBCbSoNlMpjVhT0

Import required **libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import classification_report, mean_absolute_error

"""Load the uploaded CSV file"""

df = pd.read_csv('Cyber Security .csv')

"""Clean the data"""

# Remove commas and dollar signs from 'Financial Loss' and convert to float
df['Financial Loss (in Million $)'] = df['Financial Loss (in Million $)'].str.replace('[,$]', '', regex=True).astype(float)

df['Number of Affected Users'] = df['Number of Affected Users'].str.replace(',', '').astype(int)

# Convert 'Incident Resolution Time (in Hours)' to numeric
df['Incident Resolution Time (in Hours)'] = pd.to_numeric(df['Incident Resolution Time (in Hours)'], errors='coerce')

# Fill missing resolution times with median
df['Incident Resolution Time (in Hours)'] = df['Incident Resolution Time (in Hours)'].fillna(df['Incident Resolution Time (in Hours)'].median())

# Display first few rows
print("First 5 Rows of the Dataset:")
print(df.head())

"""calculate kpis"""

total_attacks = len(df)
total_financial_loss = df['Financial Loss (in Million $)'].sum()
total_affected_users = df['Number of Affected Users'].sum()
avg_resolution_time = df['Incident Resolution Time (in Hours)'].mean()

"""Print KPIs"""

print("\nüìä Key Performance Indicators (KPIs):")
print(f"Total Number of Cyber Attacks: {total_attacks}")
print(f"Total Financial Loss (in Million $): ${total_financial_loss:.2f}M")
print(f"Total Affected Users: {total_affected_users}")
print(f"Average Incident Resolution Time: {avg_resolution_time:.2f} hours")

"""Visualize Top Countries with Most Attacks"""

plt.figure(figsize=(10,6))
sns.countplot(y='Country', data=df, order=df['Country'].value_counts().index)
plt.title('Top Countries with Most Cyber Attacks')
plt.show()

"""Distribution of Attack Types"""

plt.figure(figsize=(10,6))
sns.countplot(y='Attack Type', data=df, order=df['Attack Type'].value_counts().index)
plt.title('Distribution of Attack Types')
plt.show()

"""Machine Learning Part - Predicting Financial Loss or Attack Type

# Option A: Predict Financial Loss (Regression)
# Option B: Predict Attack Type (Classification)

# We'll demonstrate both tasks. Let's choose Regression first.

# Feature Engineering
# Select relevant **features**
"""

features = ['Country', 'Year', 'Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Defense Mechanism Used']
X = df[features]
y_regression = df['Financial Loss (in Million $)']  # Target variable for regression
y_classification = df['Attack Type']               # Target variable for classification

"""# Encode categorical variables"""

label_encoders = {}
for col in X.columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

"""# Split data into train and test sets"""

X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)
_, _, y_train_clf, y_test_clf = train_test_split(X, y_classification, test_size=0.2, random_state=42)

"""Regression - Predict Financial Loss"""

reg_model = RandomForestRegressor(n_estimators=100, random_state=42)
reg_model.fit(X_train, y_train_reg)
y_pred_reg = reg_model.predict(X_test)

mae = mean_absolute_error(y_test_reg, y_pred_reg)
print(f"\nüìâ Mean Absolute Error in Financial Loss Prediction: ${mae:.2f}M")

"""Classification - Predict Attack Type"""

clf_model = RandomForestClassifier(n_estimators=100, random_state=42)
clf_model.fit(X_train, y_train_clf)
y_pred_clf = clf_model.predict(X_test)

print("\nüîç Classification Report for Attack Type Prediction:")
print(classification_report(y_test_clf, y_pred_clf))

"""Feature Importance for Regression"""

plt.figure(figsize=(10,6))
sns.barplot(x=reg_model.feature_importances_, y=X.columns)
plt.title('Feature Importance for Financial Loss Prediction')
plt.show()

"""Advanced Machine Learning (XGBoost + GridSearchCV)

We'll focus on predicting Financial Loss (in Million $) using XGBoost Regressor , and tune it using GridSearchCV.
"""

# Step 1: Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

#Step 2: Load and clean data
df = pd.read_csv('Cyber Security .csv')

# Clean financial loss column
df['Financial Loss (in Million $)'] = df['Financial Loss (in Million $)'].str.replace('[,$]', '', regex=True).astype(float)
df['Number of Affected Users'] = df['Number of Affected Users'].str.replace(',', '').astype(int)
df['Incident Resolution Time (in Hours)'] = pd.to_numeric(df['Incident Resolution Time (in Hours)'], errors='coerce').fillna(0)

# Feature Selection
features = ['Country', 'Year', 'Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Defense Mechanism Used']
X = df[features]
y = df['Financial Loss (in Million $)']

# Encode categorical features
label_encoders = {}
for col in X.columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize XGBoost Regressor
xgb_model = XGBRegressor(random_state=42)

#Define hyperparameters to tune
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

#Grid Search
grid_search = GridSearchCV(estimator=xgb_model,
                           param_grid=param_grid,
                           scoring='neg_mean_absolute_error',
                           cv=5,
                           n_jobs=-1,
                           verbose=1)

grid_search.fit(X_train, y_train)

#Best Model
best_xgb = grid_search.best_estimator_
y_pred = best_xgb.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Mean Absolute Error after tuning: ${mae:.2f}M")

from sklearn.ensemble import IsolationForest

# Use the same cleaned feature set
iso_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)
df['anomaly_score'] = iso_forest.fit_predict(X)

# Add anomaly flag (-1 is outlier, 1 is normal)
df['is_anomaly'] = df['anomaly_score'].apply(lambda x: 'Outlier' if x == -1 else 'Normal')

# Display top anomalies
anomalies = df[df['is_anomaly'] == 'Outlier']
print("\nTop Anomalous Cyber Attacks:")
print(anomalies[['Country', 'Year', 'Attack Type', 'Financial Loss (in Million $)', 'is_anomaly']])